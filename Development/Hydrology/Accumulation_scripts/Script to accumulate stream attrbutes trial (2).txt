module load R-2.15.1

#drafted by Jeremy VanDerWal ( jjvanderwal@gmail.com ... www.jjvanderwal.com )
#GNU General Public License .. feel free to use / distribute ... no warranties

################################################################################
library(igraph); library(parallel) #load the necessary libraries

#define inputs
input.dir='/home/jc246980/Hydrology.trials/'
load(file=paste(input.dir,'Stream.accumulation.metrics.Rdata',sep=''))
colnames(Info.table)=c("SegmentNo", "Annual_runoff", "AREA", "Cleared_veg_km", "Urban_km")
attribute=Info.table[,c(1,2)] #rename object

cois=colnames(attribute)[-grep('SegmentNo',colnames(attribute))] #define a vector of your colnames of interest

#define conditions
use.proportion=TRUE
#TRUE if proportion needs to be apportioned to bifurcations. ie. runoff
#FALSE if upstream value should be the same for both bifurcations. ie. Area

#define outputs
out.dir='/home/jc246980/Hydrology.trials/' #define output directory
filename='accumulate_annual_mean_trial_1' #name your file
################################################################################
proportion = read.csv('proportion.csv',as.is=TRUE)

wd = "/home/jc165798/working/NARP_hydro/flow_accumulation/"; setwd(wd) #define and set working directory

###read in necessary data
network = read.csv('NetworkAttributes.csv',as.is=TRUE) #read in the data
proportion = read.csv('proportion.csv',as.is=TRUE)

#prepare all data
db = merge(network,proportion[,c(1,4,5)],all=TRUE) #read in proportion rules and merge with network data
attribute=as.data.frame(attribute) #convert attributes to dataframe
attribute=attribute[which(attribute$SegmentNo %in% network$SegmentNo),] #remove extra SegmentNos
attribute=na.omit(attribute)
db = merge(db,attribute,all=TRUE) #merge data into db
db[,cois]=db[,cois]*db$SegProp #Calculate local attribute attributed to each HydroID and overwrite SegNo attribute
db = db[,c(11,12,1:10,13:ncol(db))] #reorder the columns
db=db[which(is.finite(db[,cois[1]])),] #remove NAs (islands, etc)
if (use.proportion==FALSE) db$BiProp=1
rm(list=c("network","attribute","proportion")) #cleanup extra files

### create graph object and all possible subgraphs
g = graph.data.frame(db,directed=TRUE) #create the graph
gg = decompose.graph(g,"weak") #break the full graph into 10000 + subgraphs

### do the accumulation
#function to accumulate info in each subgraph in a full graph
accum = function(gt,cois) {
require(igraph)
out=NULL #define the output
while(length(E(gt))>0) { #loop until all edges are dealt with
vois = which(degree(gt,mode="in")==0) #get index of the headwater vertices
suppressWarnings({
tt = do.call("rbind", neighborhood(gt,1,V(gt)[vois],"out")) #get the index of the from & to nodes for each edge in a list
})
v.from.to = NULL; for (ii in 2:ncol(tt)) v.from.to = rbind(v.from.to,tt[,c(1,ii)]) #flatten the list to a matrix and setup for next cleaning
v.from.to = rbind(v.from.to,v.from.to) #allow for duplicate from-to nodes with separate SegmentNo
if (is.null(dim(v.from.to))) v.from.to = matrix(v.from.to,ncol=2) #ensure v.from.to is a matrix
eois = get.edge.ids(gt,t(cbind(V(gt)[v.from.to[,1]],V(gt)[v.from.to[,2]])),multi=TRUE) #get an index of the output edges from that vertex
eois = unique(eois); if (0 %in% eois) eois = eois[-which(eois==0)] #only keep unique eois
tout=E(gt)$HydroID[eois] #prepare empty df to store attributes
for (coi in cois){ tt=get.edge.attribute(gt,coi,eois); tout=cbind(tout,tt)} #store the attribute for the selected edges for each of the columns to be accumulated
colnames(tout)=c('HydroID',cois) #name the columns
out = rbind(out,tout) #store the attribute for the current edges

suppressWarnings({
tt = cbind(eois, do.call("rbind", neighborhood(gt,1,V(gt)[v.from.to[,2]],"out"))) #get the next down verticies from the current edges
})
if ((length(dim(tt))<1 & length(tt)>2) | (length(dim(tt))>0 & ncol(tt)>2)) { #only do this if there is something down stream
next_edge = NULL; for (ii in 3:ncol(tt)) next_edge = rbind(next_edge,tt[,c(1,2,ii)]) #flatten the list to a matrix and setup for next cleaning
if (is.null(dim(next_edge))) next_edge = matrix(next_edge,ncol=3) #ensure v.from.to is a matrix
next_edge = cbind(next_edge,get.edge.ids(gt,t(cbind(V(gt)[next_edge[,2]],V(gt)[next_edge[,3]])))) #get an index of the next down edges
next_edge=unique(next_edge);
next_edge=next_edge[which(next_edge[,4]>0),];next_edge=matrix(next_edge,ncol=4);
colnames(next_edge) = c("e.from","from","to","e.next")
for (coi in cois) {
v=cbind(next_edge[,'e.next'],get.edge.attribute(gt, coi, next_edge[,'e.next']) + E(gt)$BiProp[next_edge[,"e.next"]] * get.edge.attribute(gt, coi, next_edge[,'e.from'])) #creates a 2 colmn matrix of next edge id and the accumulated value -- needed to deal with 2 or more flows going into a single node
colnames(v)=c('e.next','acc')
if (nrow(v)!=length(unique(v[,1]))) { #do this if there is any duplicate net down edges
tfun = function(x) {return(c(sum(x),length(x)))} #define a aggregate function to get sums for e.next and a count
tt = aggregate(v[,2],by=list(e.next=v[,1]), tfun ) #aggregate e.next sums
tt=as.matrix(tt) #convert to matrix for indexing purposes
tt[,3] = tt[,3] -1 #we need to remove duplications of e.next flow so first remove 1 from counts so that where there is only a single flow nothing will be removed
tt[,2] = tt[,2] - tt[,3] * get.edge.attribute(gt, coi, tt[,'e.next']) #remove the number of e.next flows that it has been duplicated
v = tt #set v = tt[,1:2] as it has been corrected for flows

}
gt=set.edge.attribute(gt, coi, v[,'e.next'],v[,2])
}
}
gt = delete.vertices(gt, V(gt)[vois]) #remove the vois
}
return(out)
}

###do the actual accumulation
ncore=4 #this number of cores seems most appropriate
cl <- makeCluster(getOption("cl.cores", ncore))#define the cluster for running the analysis
print(system.time({ tout = parLapplyLB(cl,gg,accum, cois=cois) }))
stopCluster(cl) #stop the cluster for analysis

###need to store the outputs
out = do.call("rbind",tout) #aggregate the list into a single matrix
db2 = merge(db,out) #merge this back into the overall database

write.csv(out,paste(out.dir,filename,'.csv',sep=''),row.names=F)

###Bring in runoff to compare

library(maptools)
out.dir='/home/jc246980/Hydrology.trials/' #define output directory
filename='accumulate_annual_runoff' #name your file

out=read.csv(paste(out.dir,filename,'.csv',sep=''))
streamatts = read.dbf('/home/jc246980/Janet_Stein_data/streamattributes.dbf')
Annualmean = streamatts[,2:3]

networkatts = read.dbf('/home/jc246980/Janet_Stein_data/NetworkAttributes.dbf')
segment2hydroid = networkatts[,c(2,9)]

tdata=merge(out,segment2hydroid, by='HydroID')
colnames(tdata)[3]=c("SEGMENTNO")
tdata=merge(tdata,Annualmean, by='SEGMENTNO')

cor(tdata$Annual_mean, tdata$RUNNANNMEAN)









